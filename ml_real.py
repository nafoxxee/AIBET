#!/usr/bin/env python3
"""
AIBET Analytics Platform - Real ML Models
ML –º–æ–¥–µ–ª–∏ –æ–±—É—á–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
"""

import asyncio
import logging
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report
import pickle
import os
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Tuple, Any

from database import Match, Signal, db_manager
from feature_engineering import feature_engineering

logger = logging.getLogger(__name__)

class RealMLModels:
    def __init__(self):
        self.name = "Real ML Models"
        self.models_path = "models/"
        self.min_training_samples = 100
        self.confidence_threshold = 0.70
        
        # –ú–æ–¥–µ–ª–∏
        self.rf_model = None
        self.lr_model = None
        self.scaler = StandardScaler()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self._initialized = False
        self._trained = False
        self.training_stats = {}
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –º–æ–¥–µ–ª–µ–π
        os.makedirs(self.models_path, exist_ok=True)
    
    async def initialize(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ML –º–æ–¥–µ–ª–µ–π"""
        if self._initialized:
            return
        
        logger.info("ü§ñ Initializing Real ML Models")
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
            await self.load_models()
            
            if self.rf_model and self.lr_model:
                logger.info("‚úÖ Loaded existing ML models")
                self._trained = True
            else:
                logger.info("üìö No existing models found, will train when enough data available")
            
            self._initialized = True
            logger.info("‚úÖ Real ML Models initialized")
            
        except Exception as e:
            logger.error(f"‚ùå Error initializing ML models: {e}")
            self._initialized = True  # –ü–æ–º–µ—á–∞–µ–º –∫–∞–∫ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ, –Ω–æ –Ω–µ –æ–±—É—á–µ–Ω–Ω—ã–µ
    
    async def train_models(self):
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        if not self._initialized:
            await self.initialize()
        
        logger.info("üéØ Training ML models on real data")
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–µ –º–∞—Ç—á–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            finished_matches = await db_manager.get_finished_matches(limit=1000)
            
            if len(finished_matches) < self.min_training_samples:
                logger.warning(f"‚ö†Ô∏è Not enough data for training: {len(finished_matches)} matches (need {self.min_training_samples})")
                return False
            
            logger.info(f"üìä Using {len(finished_matches)} finished matches for training")
            
            # –°–æ–∑–¥–∞–µ–º –æ–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ
            X, y = await self.create_training_data(finished_matches)
            
            if len(X) < 50:
                logger.warning(f"‚ö†Ô∏è Insufficient training samples: {len(X)}")
                return False
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
            X_scaled = self.scaler.fit_transform(X)
            
            # –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ
            X_train, X_test, y_train, y_test = train_test_split(
                X_scaled, y, test_size=0.2, random_state=42, stratify=y if len(np.unique(y)) > 1 else None
            )
            
            # –û–±—É—á–∞–µ–º RandomForest
            logger.info("üå≤ Training RandomForest...")
            self.rf_model = RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                min_samples_split=5,
                min_samples_leaf=2,
                random_state=42,
                class_weight='balanced'
            )
            self.rf_model.fit(X_train, y_train)
            
            # –û—Ü–µ–Ω–∏–≤–∞–µ–º RandomForest
            rf_pred = self.rf_model.predict(X_test)
            rf_accuracy = accuracy_score(y_test, rf_pred)
            logger.info(f"üå≤ RandomForest accuracy: {rf_accuracy:.3f}")
            
            # –û–±—É—á–∞–µ–º LogisticRegression
            logger.info("üìà Training LogisticRegression...")
            self.lr_model = LogisticRegression(
                max_iter=1000,
                random_state=42,
                class_weight='balanced'
            )
            self.lr_model.fit(X_train, y_train)
            
            # –û—Ü–µ–Ω–∏–≤–∞–µ–º LogisticRegression
            lr_pred = self.lr_model.predict(X_test)
            lr_accuracy = accuracy_score(y_test, lr_pred)
            logger.info(f"üìà LogisticRegression accuracy: {lr_accuracy:.3f}")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª–∏
            await self.save_models()
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            self.training_stats = {
                'samples_count': len(X),
                'rf_accuracy': rf_accuracy,
                'lr_accuracy': lr_accuracy,
                'training_date': datetime.now().isoformat(),
                'feature_count': X.shape[1]
            }
            
            self._trained = True
            logger.info(f"‚úÖ Models trained successfully. RF: {rf_accuracy:.3f}, LR: {lr_accuracy:.3f}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Error training models: {e}")
            return False
    
    async def create_training_data(self, matches: List[Match]) -> Tuple[np.ndarray, np.ndarray]:
        """–°–æ–∑–¥–∞–Ω–∏–µ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ –º–∞—Ç—á–µ–π"""
        X = []
        y = []
        
        for match in matches:
            try:
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –º–∞—Ç—á–∏ –±–µ–∑ —Å—á–µ—Ç–∞
                if not match.score or ':' not in match.score:
                    continue
                
                # –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
                features = await feature_engineering.create_match_features(match)
                feature_vector = feature_engineering.get_feature_vector(features)
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                score_parts = match.score.split(':')
                if len(score_parts) >= 2:
                    try:
                        score1 = int(score_parts[0])
                        score2 = int(score_parts[1])
                        result = 1 if score1 > score2 else 0
                    except:
                        continue
                    
                    X.append(feature_vector)
                    y.append(result)
                    
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Error processing match {match.id}: {e}")
                continue
        
        return np.array(X), np.array(y)
    
    async def predict_match(self, match: Match) -> Optional[Dict[str, Any]]:
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –º–∞—Ç—á–∞"""
        if not self._trained or not self.rf_model or not self.lr_model:
            return None
        
        try:
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
            features = await feature_engineering.create_match_features(match)
            feature_vector = feature_engineering.get_feature_vector(features)
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º
            feature_vector_scaled = self.scaler.transform([feature_vector])
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            rf_pred = self.rf_model.predict_proba(feature_vector_scaled)[0]
            lr_pred = self.lr_model.predict_proba(feature_vector_scaled)[0]
            
            # –ê–Ω—Å–∞–º–±–ª—å
            ensemble_pred = (rf_pred + lr_pred) / 2
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
            if ensemble_pred[1] > ensemble_pred[0]:
                prediction = f"{match.team1} –ø–æ–±–µ–¥–∏—Ç"
                confidence = ensemble_pred[1]
            else:
                prediction = f"{match.team2} –ø–æ–±–µ–¥–∏—Ç"
                confidence = ensemble_pred[0]
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
            result = {
                'prediction': prediction,
                'confidence': float(confidence),
                'rf_confidence': float(rf_pred[1] if rf_pred[1] > rf_pred[0] else rf_pred[0]),
                'lr_confidence': float(lr_pred[1] if lr_pred[1] > lr_pred[0] else lr_pred[0]),
                'team1_win_prob': float(ensemble_pred[1]),
                'team2_win_prob': float(ensemble_pred[0]),
                'model_type': 'ensemble_rf_lr',
                'features_used': len(feature_vector),
                'prediction_time': datetime.now().isoformat()
            }
            
            # –î–æ–±–∞–≤–ª—è–µ–º –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ
            result['explanation'] = self._generate_explanation(features, result)
            
            return result
            
        except Exception as e:
            logger.error(f"‚ùå Error predicting match {match.team1} vs {match.team2}: {e}")
            return None
    
    def _generate_explanation(self, features: Dict[str, Any], prediction: Dict[str, Any]) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–±—ä—è—Å–Ω–µ–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è"""
        try:
            explanations = []
            
            # Winrate —Ä–∞–∑–Ω–∏—Ü–∞
            winrate_diff = features.get('winrate_diff', 0)
            if abs(winrate_diff) > 10:
                if winrate_diff > 0:
                    explanations.append(f"–ü–µ—Ä–≤–∞—è –∫–æ–º–∞–Ω–¥–∞ –∏–º–µ–µ—Ç winrate –≤—ã—à–µ –Ω–∞ {abs(winrate_diff):.1f}%")
                else:
                    explanations.append(f"–í—Ç–æ—Ä–∞—è –∫–æ–º–∞–Ω–¥–∞ –∏–º–µ–µ—Ç winrate –≤—ã—à–µ –Ω–∞ {abs(winrate_diff):.1f}%")
            
            # –§–æ—Ä–º–∞
            form_diff = features.get('form_diff', 0)
            if abs(form_diff) > 1:
                if form_diff > 0:
                    explanations.append("–ü–µ—Ä–≤–∞—è –∫–æ–º–∞–Ω–¥–∞ –≤ –ª—É—á—à–µ–π —Ñ–æ—Ä–º–µ")
                else:
                    explanations.append("–í—Ç–æ—Ä–∞—è –∫–æ–º–∞–Ω–¥–∞ –≤ –ª—É—á—à–µ–π —Ñ–æ—Ä–º–µ")
            
            # H2H –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ
            h2h_adv = features.get('h2h_team1_advantage', 0.5)
            if h2h_adv > 0.7:
                explanations.append("–ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏ –¥–æ–º–∏–Ω–∏—Ä—É–µ—Ç –≤ –ª–∏—á–Ω—ã—Ö –≤—Å—Ç—Ä–µ—á–∞—Ö")
            elif h2h_adv < 0.3:
                explanations.append("–ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏ —É—Å—Ç—É–ø–∞–µ—Ç –≤ –ª–∏—á–Ω—ã—Ö –≤—Å—Ç—Ä–µ—á–∞—Ö")
            
            # –í–∞–∂–Ω–æ—Å—Ç—å —Ç—É—Ä–Ω–∏—Ä–∞
            importance = features.get('importance', 5)
            if importance >= 8:
                explanations.append("–ú–∞—Ç—á –≤—ã—Å–æ–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è (–≤–∞–∂–Ω—ã–π —Ç—É—Ä–Ω–∏—Ä)")
            
            return " | ".join(explanations) if explanations else "–ë–∞–ª–∞–Ω—Å —Å–∏–ª"
            
        except:
            return "–ê–Ω–∞–ª–∏–∑ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫–æ–º–∞–Ω–¥"
    
    async def save_models(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π"""
        try:
            # RandomForest
            with open(os.path.join(self.models_path, 'rf_model.pkl'), 'wb') as f:
                pickle.dump(self.rf_model, f)
            
            # LogisticRegression
            with open(os.path.join(self.models_path, 'lr_model.pkl'), 'wb') as f:
                pickle.dump(self.lr_model, f)
            
            # Scaler
            with open(os.path.join(self.models_path, 'scaler.pkl'), 'wb') as f:
                pickle.dump(self.scaler, f)
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            with open(os.path.join(self.models_path, 'training_stats.json'), 'w') as f:
                import json
                json.dump(self.training_stats, f, indent=2)
            
            logger.info("üíæ Models saved successfully")
            
        except Exception as e:
            logger.error(f"‚ùå Error saving models: {e}")
    
    async def load_models(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π"""
        try:
            # RandomForest
            rf_path = os.path.join(self.models_path, 'rf_model.pkl')
            if os.path.exists(rf_path):
                with open(rf_path, 'rb') as f:
                    self.rf_model = pickle.load(f)
            
            # LogisticRegression
            lr_path = os.path.join(self.models_path, 'lr_model.pkl')
            if os.path.exists(lr_path):
                with open(lr_path, 'rb') as f:
                    self.lr_model = pickle.load(f)
            
            # Scaler
            scaler_path = os.path.join(self.models_path, 'scaler.pkl')
            if os.path.exists(scaler_path):
                with open(scaler_path, 'rb') as f:
                    self.scaler = pickle.load(f)
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            stats_path = os.path.join(self.models_path, 'training_stats.json')
            if os.path.exists(stats_path):
                with open(stats_path, 'r') as f:
                    import json
                    self.training_stats = json.load(f)
            
            logger.info("üìö Models loaded successfully")
            
        except Exception as e:
            logger.error(f"‚ùå Error loading models: {e}")
    
    def get_model_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –º–æ–¥–µ–ª–µ–π"""
        return {
            'initialized': self._initialized,
            'trained': self._trained,
            'training_stats': self.training_stats,
            'confidence_threshold': self.confidence_threshold,
            'min_training_samples': self.min_training_samples
        }

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä
real_ml_models = RealMLModels()
