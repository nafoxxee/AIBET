#!/usr/bin/env python3
"""
AIBET Analytics Platform - ML Models
RandomForestClassifier + LogisticRegression –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –º–∞—Ç—á–µ–π
"""

import asyncio
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report
import pickle
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Tuple, Any

from database import Match, Signal, db_manager

logger = logging.getLogger(__name__)

class AdvancedMLModels:
    def __init__(self, db_manager=None):
        self.db_manager = db_manager
        self.rf_model = None
        self.lr_model = None
        self.scaler = StandardScaler()
        self.feature_columns = [
            'rating_diff',
            'home_advantage',
            'tournament_importance',
            'stage_importance',
            'format_importance',
            'team1_form',
            'team2_form',
            'h2h_advantage'
        ]
        self.models_path = "models/"
        self._initialized = False
    
    async def initialize(self):
        """–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ML –º–æ–¥–µ–ª–µ–π"""
        if self._initialized:
            return
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –ë–î
        if not self.db_manager:
            logger.warning("‚ö†Ô∏è DB not initialized, skipping ML init")
            self._initialized = True  # –ü–æ–º–µ—á–∞–µ–º –∫–∞–∫ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π, –Ω–æ –±–µ–∑ –º–æ–¥–µ–ª–µ–π
            return
            
        logger.info("ü§ñ Initializing ML Models")
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –º–æ–¥–µ–ª–µ–π
        import os
        os.makedirs(self.models_path, exist_ok=True)
        
        try:
            # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–æ–¥–µ–ª–∏
            await self.load_models()
            
            if self.rf_model is None or self.lr_model is None:
                logger.info("üìö No existing models found, will train later")
                # –ù–ï –æ–±—É—á–∞–µ–º —Å—Ä–∞–∑—É, –∞ –æ—Ç–∫–ª–∞–¥—ã–≤–∞–µ–º –Ω–∞ —Ñ–æ–Ω
                self._initialized = True
                logger.info("‚úÖ ML Models initialized (training scheduled for background)")
            else:
                logger.info("‚úÖ Existing models loaded successfully")
                self._initialized = True
                logger.info("‚úÖ ML Models initialized successfully")
            
        except Exception as e:
            logger.exception(f"‚ùå Error initializing ML models: {e}")
            # –ù–ï –ø–∞–¥–∞–µ–º, –∞ –ø–æ–º–µ—á–∞–µ–º –∫–∞–∫ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –±–µ–∑ –º–æ–¥–µ–ª–µ–π
            self._initialized = True
            logger.warning("‚ö†Ô∏è ML Models initialized without training (will retry later)")
    
    def extract_features(self, match: Match) -> np.ndarray:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –º–∞—Ç—á–∞"""
        features = match.features or {}
        
        # –†–µ–π—Ç–∏–Ω–≥–æ–≤–∞—è —Ä–∞–∑–Ω–∏—Ü–∞
        rating_diff = features.get('rating_diff', 0)
        if rating_diff is None:
            rating_diff = 0
        
        # –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ –¥–æ–º–∞—à–Ω–µ–π –ø–ª–æ—â–∞–¥–∫–∏
        home_advantage = 1 if features.get('home_advantage', False) else 0
        
        # –í–∞–∂–Ω–æ—Å—Ç—å —Ç—É—Ä–Ω–∏—Ä–∞
        tournament = features.get('tournament', '').lower()
        tournament_importance = 0
        if any(word in tournament for word in ['final', 'playoff', 'championship']):
            tournament_importance = 3
        elif any(word in tournament for word in ['semifinal', 'quarterfinal']):
            tournament_importance = 2
        elif 'regular' in tournament:
            tournament_importance = 1
        
        # –í–∞–∂–Ω–æ—Å—Ç—å —Å—Ç–∞–¥–∏–∏
        stage = features.get('stage', '').lower()
        stage_importance = 0
        if 'final' in stage:
            stage_importance = 3
        elif 'semifinal' in stage:
            stage_importance = 2
        elif 'playoff' in stage:
            stage_importance = 2
        elif 'group' in stage:
            stage_importance = 1
        
        # –í–∞–∂–Ω–æ—Å—Ç—å —Ñ–æ—Ä–º–∞—Ç–∞
        format_type = features.get('format', '').upper()
        format_importance = 0
        if 'BO5' in format_type:
            format_importance = 3
        elif 'BO3' in format_type:
            format_importance = 2
        elif 'BO1' in format_type:
            format_importance = 1
        
        # –§–æ—Ä–º–∞ –∫–æ–º–∞–Ω–¥ (—Å–∏–º—É–ª—è—Ü–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–π—Ç–∏–Ω–≥–∞)
        team1_rating = features.get('team1_rating', 50)
        team2_rating = features.get('team2_rating', 50)
        
        # –§–æ—Ä–º–∞ –∫–æ–º–∞–Ω–¥—ã 1 (–æ—Å–Ω–æ–≤–∞–Ω–∞ –Ω–∞ —Ä–µ–π—Ç–∏–Ω–≥–µ)
        team1_form = min(max(team1_rating / 20, 1), 5)  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º 1-5
        
        # –§–æ—Ä–º–∞ –∫–æ–º–∞–Ω–¥—ã 2 (–æ—Å–Ω–æ–≤–∞–Ω–∞ –Ω–∞ —Ä–µ–π—Ç–∏–Ω–≥–µ)
        team2_form = min(max(team2_rating / 20, 1), 5)  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º 1-5
        
        # H2H –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ (—Å–∏–º—É–ª—è—Ü–∏—è)
        h2h_advantage = np.random.normal(0, 0.5)  # –°–ª—É—á–∞–π–Ω–æ–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ
        
        feature_vector = np.array([
            rating_diff,
            home_advantage,
            tournament_importance,
            stage_importance,
            format_importance,
            team1_form,
            team2_form,
            h2h_advantage
        ])
        
        return feature_vector
    
    def create_training_data(self, matches: List[Match]) -> Tuple[np.ndarray, np.ndarray]:
        """–°–æ–∑–¥–∞–Ω–∏–µ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ –º–∞—Ç—á–µ–π"""
        X = []
        y = []
        
        for match in matches:
            if match.status != 'finished' or not match.score:
                continue
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
            features = self.extract_features(match)
            X.append(features)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç (1 - –ø–æ–±–µ–¥–∞ –∫–æ–º–∞–Ω–¥—ã 1, 0 - –ø–æ–±–µ–¥–∞ –∫–æ–º–∞–Ω–¥—ã 2)
            try:
                score_parts = match.score.split(':')
                if len(score_parts) >= 2:
                    score1 = int(score_parts[0])
                    score2 = int(score_parts[1])
                    result = 1 if score1 > score2 else 0
                    y.append(result)
            except:
                continue
        
        return np.array(X), np.array(y)
    
    async def train_models(self):
        """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ ML –º–æ–¥–µ–ª–µ–π"""
        logger.info("üéØ Training ML Models")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –ë–î
        if not self.db_manager:
            logger.warning("‚ö†Ô∏è DB not initialized, skipping ML training")
            return
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–µ –º–∞—Ç—á–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            matches = await self.db_manager.get_matches(status="finished", limit=1000)
            
            if len(matches) < 100:
                logger.warning(f"‚ö†Ô∏è Not enough data for ML training: {len(matches)} matches (need 100+)")
                # –°–æ–∑–¥–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –±–∞–∑–æ–≤–æ–π —Ä–∞–±–æ—Ç—ã
                logger.info("üìö Creating synthetic data for basic ML functionality")
                X, y = self.create_synthetic_data()
            else:
                logger.info(f"üìö Using {len(matches)} matches for training")
                X, y = self.create_training_data(matches)
            
            if len(X) < 20:
                logger.warning(f"‚ö†Ô∏è Insufficient training data: {len(X)} samples")
                return
            
            logger.info(f"üìä Training with {len(X)} samples")
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
            X_scaled = self.scaler.fit_transform(X)
            
            # –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ
            X_train, X_test, y_train, y_test = train_test_split(
                X_scaled, y, test_size=0.2, random_state=42, stratify=y if len(np.unique(y)) > 1 else None
            )
            
            # –û–±—É—á–∞–µ–º RandomForest
            logger.info("üå≤ Training RandomForest...")
            self.rf_model = RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                min_samples_split=5,
                min_samples_leaf=2,
                random_state=42
            )
            self.rf_model.fit(X_train, y_train)
            
            # –û—Ü–µ–Ω–∏–≤–∞–µ–º RandomForest
            rf_pred = self.rf_model.predict(X_test)
            rf_accuracy = accuracy_score(y_test, rf_pred)
            logger.info(f"üå≤ RandomForest accuracy: {rf_accuracy:.3f}")
            
            # –û–±—É—á–∞–µ–º LogisticRegression
            logger.info("üìà Training LogisticRegression...")
            self.lr_model = LogisticRegression(
                max_iter=1000,
                random_state=42,
                class_weight='balanced'
            )
            self.lr_model.fit(X_train, y_train)
            
            # –û—Ü–µ–Ω–∏–≤–∞–µ–º LogisticRegression
            lr_pred = self.lr_model.predict(X_test)
            lr_accuracy = accuracy_score(y_test, lr_pred)
            logger.info(f"üìà LogisticRegression accuracy: {lr_accuracy:.3f}")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª–∏
            await self.save_models()
            
            logger.info("‚úÖ ML Models trained successfully")
            
        except Exception as e:
            logger.exception(f"‚ùå Error training models: {e}")
            # –ù–ï –ø–∞–¥–∞–µ–º, –∞ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º —Ä–∞–±–æ—Ç—É
            logger.warning("‚ö†Ô∏è ML training failed, continuing without models")
    
    async def save_models(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –≤ —Ñ–∞–π–ª—ã"""
        try:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º RandomForest
            rf_path = f"{self.models_path}random_forest.pkl"
            with open(rf_path, 'wb') as f:
                pickle.dump(self.rf_model, f)
            logger.info(f"üíæ RandomForest saved to {rf_path}")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º LogisticRegression
            lr_path = f"{self.models_path}logistic_regression.pkl"
            with open(lr_path, 'wb') as f:
                pickle.dump(self.lr_model, f)
            logger.info(f"üíæ LogisticRegression saved to {lr_path}")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º scaler
            scaler_path = f"{self.models_path}scaler.pkl"
            with open(scaler_path, 'wb') as f:
                pickle.dump(self.scaler, f)
            logger.info(f"üíæ Scaler saved to {scaler_path}")
            
        except Exception as e:
            logger.exception(f"‚ùå Error saving models: {e}")
            raise
    
    async def load_models(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π –∏–∑ —Ñ–∞–π–ª–æ–≤"""
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º RandomForest
            rf_path = f"{self.models_path}random_forest.pkl"
            if os.path.exists(rf_path):
                with open(rf_path, 'rb') as f:
                    self.rf_model = pickle.load(f)
                logger.info(f"üìÇ RandomForest loaded from {rf_path}")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º LogisticRegression
            lr_path = f"{self.models_path}logistic_regression.pkl"
            if os.path.exists(lr_path):
                with open(lr_path, 'rb') as f:
                    self.lr_model = pickle.load(f)
                logger.info(f"üìÇ LogisticRegression loaded from {lr_path}")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º scaler
            scaler_path = f"{self.models_path}scaler.pkl"
            if os.path.exists(scaler_path):
                with open(scaler_path, 'rb') as f:
                    self.scaler = pickle.load(f)
                logger.info(f"üìÇ Scaler loaded from {scaler_path}")
                
        except Exception as e:
            logger.exception(f"‚ùå Error loading models: {e}")
            # –ü—Ä–∏ –æ—à–∏–±–∫–µ —Å–±—Ä–∞—Å—ã–≤–∞–µ–º –º–æ–¥–µ–ª–∏
            self.rf_model = None
            self.lr_model = None
    
    def create_synthetic_data(self) -> Tuple[np.ndarray, np.ndarray]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        n_samples = 500
        X = []
        y = []
        
        for i in range(n_samples):
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            rating_diff = np.random.normal(0, 20)
            home_advantage = np.random.choice([0, 1])
            tournament_importance = np.random.choice([0, 1, 2, 3])
            stage_importance = np.random.choice([0, 1, 2, 3])
            format_importance = np.random.choice([0, 1, 2, 3])
            team1_form = np.random.uniform(1, 5)
            team2_form = np.random.uniform(1, 5)
            h2h_advantage = np.random.normal(0, 0.5)
            
            features = np.array([
                rating_diff, home_advantage, tournament_importance,
                stage_importance, format_importance, team1_form,
                team2_form, h2h_advantage
            ])
            
            X.append(features)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å –Ω–µ–∫–æ—Ç–æ—Ä–æ–π –ª–æ–≥–∏–∫–æ–π
            score = (rating_diff * 0.3 + 
                    home_advantage * 0.2 + 
                    (team1_form - team2_form) * 0.3 +
                    np.random.normal(0, 1))
            
            result = 1 if score > 0 else 0
            y.append(result)
        
        return np.array(X), np.array(y)
    
    def create_basic_models(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        logger.info("Creating basic ML models")
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç—ã–µ –º–æ–¥–µ–ª–∏ —Å –±–∞–∑–æ–≤—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        self.rf_model = RandomForestClassifier(
            n_estimators=50,
            max_depth=5,
            random_state=42
        )
        
        self.lr_model = LogisticRegression(
            random_state=42,
            max_iter=500
        )
        
        # –û–±—É—á–∞–µ–º –Ω–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        X, y = self.create_synthetic_data()
        X_scaled = self.scaler.fit_transform(X)
        
        self.rf_model.fit(X_scaled, y)
        self.lr_model.fit(X_scaled, y)
    
    async def predict_match(self, match: Match) -> Optional[Dict]:
        """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –º–∞—Ç—á–∞"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –¥–æ—Å—Ç—É–ø–Ω—ã –ª–∏ –º–æ–¥–µ–ª–∏
            if not self.rf_model or not self.lr_model:
                logger.debug("‚ö†Ô∏è ML models not ready for prediction")
                return None
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
            features = self.extract_features(match)
            features_scaled = self.scaler.transform([features])
            
            # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            rf_pred = self.rf_model.predict(features_scaled)[0]
            rf_proba = self.rf_model.predict_proba(features_scaled)[0]
            
            lr_pred = self.lr_model.predict(features_scaled)[0]
            lr_proba = self.lr_model.predict_proba(features_scaled)[0]
            
            # –£—Å—Ä–µ–¥–Ω—è–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
            confidence = (rf_proba.max() + lr_proba.max()) / 2
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            prediction = "Team 1" if rf_pred == 1 else "Team 2"
            
            return {
                "prediction": prediction,
                "confidence": confidence,
                "rf_confidence": rf_proba.max(),
                "lr_confidence": lr_proba.max()
            }
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Error in ML prediction: {e}")
            return None
    
    def generate_analysis(self, match: Match, confidence: float, feature_importance: Dict[str, float]) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
        analysis_parts = []
        
        # –ê–Ω–∞–ª–∏–∑ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        if confidence >= 0.75:
            analysis_parts.append("–í—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏")
        elif confidence >= 0.65:
            analysis_parts.append("–°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏")
        else:
            analysis_parts.append("–ù–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å, —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –æ—Å—Ç–æ—Ä–æ–∂–Ω–æ—Å—Ç—å")
        
        # –ê–Ω–∞–ª–∏–∑ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤
        rating_diff = match.features.get('rating_diff', 0) if match.features else 0
        if rating_diff > 10:
            analysis_parts.append(f"–ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ {match.team1} –ø–æ —Ä–µ–π—Ç–∏–Ω–≥—É")
        elif rating_diff < -10:
            analysis_parts.append(f"–ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ {match.team2} –ø–æ —Ä–µ–π—Ç–∏–Ω–≥—É")
        
        # –ê–Ω–∞–ª–∏–∑ —Ñ–æ—Ä–º–∞—Ç–∞
        format_type = match.features.get('format', '') if match.features else ''
        if 'BO3' in format_type or 'BO5' in format_type:
            analysis_parts.append("–î–ª–∏–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–∞–µ—Ç –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ –±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ–π –∫–æ–º–∞–Ω–¥–µ")
        
        # –ê–Ω–∞–ª–∏–∑ —Ç—É—Ä–Ω–∏—Ä–∞
        tournament = match.features.get('tournament', '') if match.features else ''
        if any(word in tournament.lower() for word in ['final', 'playoff']):
            analysis_parts.append("–ú–∞—Ç—á –ø–ª–µ–π-–æ—Ñ—Ñ –ø–æ–≤—ã—à–∞–µ—Ç –º–æ—Ç–∏–≤–∞—Ü–∏—é –∫–æ–º–∞–Ω–¥")
        
        return ". ".join(analysis_parts) + "."
    
    async def generate_signals(self, min_confidence: float = 0.70) -> List[Signal]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∏–≥–Ω–∞–ª–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ ML –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π"""
        logger.info(f"üéØ Generating signals with min confidence: {min_confidence}")
        
        signals = []
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å—Ç–æ—è—â–∏–µ –º–∞—Ç—á–∏
            matches = await self.db_manager.get_matches(status="upcoming", limit=50)
            
            for match in matches:
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –º–∞—Ç—á–∏, –∫–æ—Ç–æ—Ä—ã–µ —Å–∫–æ—Ä–æ –Ω–∞—á–Ω—É—Ç—Å—è
                if match.start_time and match.start_time < datetime.now() + timedelta(minutes=30):
                    continue
                
                # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
                prediction = await self.predict_match(match)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
                if prediction['confidence'] >= min_confidence:
                    # –°–æ–∑–¥–∞–µ–º —Å–∏–≥–Ω–∞–ª
                    signal_text = f"AIBET AI SIGNAL üéØ\n"
                    signal_text += f"–ú–∞—Ç—á: {match.team1} vs {match.team2}\n"
                    signal_text += f"–°—Ç–∞–≤–∫–∞: –ü–æ–±–µ–¥–∞ {prediction['prediction']}\n"
                    signal_text += f"–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: {prediction['confidence']:.1%}\n"
                    signal_text += f"AI-–∞–Ω–∞–ª–∏–∑: {prediction['analysis']}"
                    
                    signal = Signal(
                        match_id=match.id,
                        sport=match.sport,
                        signal=signal_text,
                        confidence=prediction['confidence'],
                        published=False
                    )
                    
                    signals.append(signal)
            
            logger.info(f"üéØ Generated {len(signals)} signals")
            return signals
            
        except Exception as e:
            logger.error(f"Error generating signals: {e}")
            return []
    
    async def save_models(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –≤ —Ñ–∞–π–ª—ã"""
        try:
            import os
            os.makedirs(self.models_path, exist_ok=True)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º RandomForest
            with open(f"{self.models_path}/random_forest.pkl", 'wb') as f:
                pickle.dump(self.rf_model, f)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º LogisticRegression
            with open(f"{self.models_path}/logistic_regression.pkl", 'wb') as f:
                pickle.dump(self.lr_model, f)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º scaler
            with open(f"{self.models_path}/scaler.pkl", 'wb') as f:
                pickle.dump(self.scaler, f)
            
            logger.info("üíæ Models saved successfully")
            
        except Exception as e:
            logger.error(f"Error saving models: {e}")
    
    async def load_models(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π –∏–∑ —Ñ–∞–π–ª–æ–≤"""
        try:
            import os
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º RandomForest
            rf_path = f"{self.models_path}/random_forest.pkl"
            if os.path.exists(rf_path):
                with open(rf_path, 'rb') as f:
                    self.rf_model = pickle.load(f)
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º LogisticRegression
            lr_path = f"{self.models_path}/logistic_regression.pkl"
            if os.path.exists(lr_path):
                with open(lr_path, 'rb') as f:
                    self.lr_model = pickle.load(f)
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º scaler
            scaler_path = f"{self.models_path}/scaler.pkl"
            if os.path.exists(scaler_path):
                with open(scaler_path, 'rb') as f:
                    self.scaler = pickle.load(f)
            
            # –ï—Å–ª–∏ –º–æ–¥–µ–ª–µ–π –Ω–µ—Ç, –æ–±—É—á–∞–µ–º –∏—Ö
            if self.rf_model is None or self.lr_model is None:
                await self.train_models()
            
            logger.info("üìÇ Models loaded successfully")
            
        except Exception as e:
            logger.error(f"Error loading models: {e}")
            await self.train_models()

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä ML –º–æ–¥–µ–ª–µ–π
ml_models = AdvancedMLModels()
