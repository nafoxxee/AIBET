#!/usr/bin/env python3
"""
AIBET KHL Parser - Fixed Version
HTML –ø–∞—Ä—Å–∏–Ω–≥ —Å Livesport –∏ KHL.ru
–° –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º –≤ SQLite
"""

import asyncio
import aiohttp
import sqlite3
import json
import logging
import random
import time
import os
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Any
from bs4 import BeautifulSoup
import re

logger = logging.getLogger(__name__)

class KHLParserFixed:
    def __init__(self):
        self.db_path = "data/khl_cache.db"
        self.session_timeout = aiohttp.ClientTimeout(total=15)
        self.max_retries = 3
        self.cache_hours = 6  # –ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ 6 —á–∞—Å–æ–≤
        
        # User-Agent –¥–ª—è –æ–±—Ö–æ–¥–∞ –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        ]
        
        # –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –ö–•–õ
        self.sources = {
            'livesport': {
                'url': 'https://www.livesport.com/ru/hockey/russia/khl/',
                'priority': 1,
                'enabled': True
            },
            'khl_official': {
                'url': 'https://khl.ru/calendar/',
                'priority': 2,
                'enabled': True
            }
        }
        
        # –¢–æ–ø –ö–•–õ –∫–æ–º–∞–Ω–¥—ã
        self.khl_teams = [
            'CSKA Moscow', 'SKA Saint Petersburg', 'Ak Bars Kazan', 'Metallurg Magnitogorsk',
            'Salavat Yulaev Ufa', ' Lokomotiv Yaroslavl', 'Barys Nur-Sultan', 'Traktor Chelyabinsk',
            'Avangard Omsk', 'Dinamo Moscow', 'Dinamo Minsk', 'Dinamo Riga',
            'Jokerit Helsinki', 'Severstal Cherepovets', 'Neftekhimik Nizhnekamsk',
            'Vityaz Podolsk', 'Sibir Novosibirsk', 'Amur Khabarovsk', 'Admiral Vladivostok',
            'Kunlun Red Star Beijing', 'HC Sochi', 'Torpedo Nizhny Novgorod'
        ]
        
        self.session = None
        self._init_cache_db()
    
    def _init_cache_db(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è"""
        os.makedirs("data", exist_ok=True)
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS cache (
                source TEXT PRIMARY KEY,
                data TEXT,
                timestamp DATETIME,
                expires_at DATETIME
            )
        ''')
        
        conn.commit()
        conn.close()
        logger.info("‚úÖ KHL cache database initialized")
    
    def _get_cached_data(self, source: str) -> Optional[Dict]:
        """–ü–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ –∫–µ—à–∞"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT data, expires_at FROM cache 
            WHERE source = ? AND expires_at > datetime('now')
        ''', (source,))
        
        result = cursor.fetchone()
        conn.close()
        
        if result:
            logger.info(f"üì¶ Using cached data for {source}")
            return json.loads(result[0])
        
        return None
    
    def _save_cached_data(self, source: str, data: Dict):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –≤ –∫–µ—à"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        expires_at = datetime.now() + timedelta(hours=self.cache_hours)
        
        cursor.execute('''
            INSERT OR REPLACE INTO cache (source, data, timestamp, expires_at)
            VALUES (?, ?, ?, ?)
        ''', (source, json.dumps(data), datetime.now(), expires_at))
        
        conn.commit()
        conn.close()
        logger.info(f"üíæ Cached data for {source} (expires: {expires_at})")
    
    def get_headers(self) -> Dict[str, str]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ª—É—á–∞–π–Ω—ã–µ headers"""
        return {
            'User-Agent': random.choice(self.user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Cache-Control': 'max-age=0'
        }
    
    async def fetch_page(self, url: str, source_name: str) -> Optional[str]:
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å retry –∏ –∑–∞–¥–µ—Ä–∂–∫–æ–π"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–µ—à
        cached = self._get_cached_data(source_name)
        if cached:
            return cached.get('html')
        
        for attempt in range(self.max_retries):
            try:
                # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                await asyncio.sleep(random.uniform(2, 4))
                
                headers = self.get_headers()
                
                async with self.session.get(url, headers=headers, timeout=self.session_timeout) as response:
                    if response.status == 200:
                        html = await response.text()
                        
                        # –ö–µ—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                        self._save_cached_data(source_name, {'html': html})
                        
                        logger.info(f"‚úÖ Successfully fetched {source_name}")
                        return html
                    elif response.status == 403:
                        logger.warning(f"‚ö†Ô∏è 403 Forbidden for {source_name} (attempt {attempt + 1})")
                        if attempt < self.max_retries - 1:
                            await asyncio.sleep(5)
                            continue
                    elif response.status == 429:
                        logger.warning(f"‚ö†Ô∏è 429 Rate Limited for {source_name} (attempt {attempt + 1})")
                        if attempt < self.max_retries - 1:
                            await asyncio.sleep(10)
                            continue
                    else:
                        logger.warning(f"‚ö†Ô∏è HTTP {response.status} for {source_name}")
                        
            except asyncio.TimeoutError:
                logger.warning(f"‚ö†Ô∏è Timeout for {source_name} (attempt {attempt + 1})")
                if attempt < self.max_retries - 1:
                    await asyncio.sleep(3)
                    continue
            except Exception as e:
                logger.error(f"‚ùå Error fetching {source_name}: {e}")
                if attempt < self.max_retries - 1:
                    await asyncio.sleep(2)
                    continue
        
        logger.error(f"‚ùå Failed to fetch {source_name} after {self.max_retries} attempts")
        return None
    
    def is_khl_team(self, team_name: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å, —á—Ç–æ –∫–æ–º–∞–Ω–¥–∞ –ö–•–õ"""
        team_lower = team_name.lower()
        for khl_team in self.khl_teams:
            if khl_team.lower() in team_lower:
                return True
        return False
    
    def parse_livesport_matches(self, html: str) -> List[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –º–∞—Ç—á–µ–π —Å Livesport"""
        matches = []
        
        try:
            soup = BeautifulSoup(html, 'html.parser')
            
            # Livesport –∏—Å–ø–æ–ª—å–∑—É–µ—Ç div —Å –∫–ª–∞—Å—Å–æ–º event__match
            match_elements = soup.find_all('div', class_='event__match')
            
            for element in match_elements:
                try:
                    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–º–∞–Ω–¥
                    team_elements = element.find_all('div', class_='event__participant')
                    if len(team_elements) < 2:
                        continue
                    
                    team1 = team_elements[0].get_text(strip=True)
                    team2 = team_elements[1].get_text(strip=True)
                    
                    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ö–•–õ –∫–æ–º–∞–Ω–¥
                    if not (self.is_khl_team(team1) or self.is_khl_team(team2)):
                        continue
                    
                    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏
                    time_elem = element.find('div', class_='event__time')
                    match_time = time_elem.get_text(strip=True) if time_elem else ''
                    
                    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞—Ç—ã
                    date_elem = element.find('div', class_='event__date')
                    match_date = date_elem.get_text(strip=True) if date_elem else ''
                    
                    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞
                    status_elem = element.find('div', class_='event__stage')
                    status = status_elem.get_text(strip=True) if status_elem else 'upcoming'
                    
                    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—á–µ—Ç–∞
                    score_elem = element.find('div', class_='event__score')
                    score = score_elem.get_text(strip=True) if score_elem else ''
                    
                    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç—É—Ä–Ω–∏—Ä–∞
                    tournament_elem = element.find('div', class_='event__title')
                    tournament = tournament_elem.get_text(strip=True) if tournament_elem else 'KHL'
                    
                    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Å—ã–ª–∫–∏
                    link_elem = element.find('a', href=True)
                    match_link = link_elem['href'] if link_elem else ''
                    
                    match_data = {
                        'match_id': f"livesport_{hash(team1 + team2 + match_time)}",
                        'team1': team1,
                        'team2': team2,
                        'tournament': tournament,
                        'sport': 'khl',
                        'date': f"{match_date} {match_time}".strip(),
                        'status': 'live' if 'live' in status.lower() else ('finished' if score else 'upcoming'),
                        'score': score if score else None,
                        'link': f"https://www.livesport.com{match_link}" if match_link.startswith('/') else match_link,
                        'source': 'livesport'
                    }
                    matches.append(match_data)
                    
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Error parsing livesport match: {e}")
                    continue
                    
        except Exception as e:
            logger.error(f"‚ùå Error parsing livesport matches: {e}")
        
        logger.info(f"üìä Livesport: Found {len(matches)} KHL matches")
        return matches
    
    def parse_khl_official_matches(self, html: str) -> List[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –º–∞—Ç—á–µ–π —Å –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–≥–æ —Å–∞–π—Ç–∞ –ö–•–õ"""
        matches = []
        
        try:
            soup = BeautifulSoup(html, 'html.parser')
            
            # –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç –ö–•–õ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–∞–±–ª–∏—Ü—ã
            match_table = soup.find('table', class_='schedule')
            if not match_table:
                # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫
                match_table = soup.find('div', class_='calendar')
            
            if match_table:
                rows = match_table.find_all('tr')
                
                for row in rows:
                    try:
                        cells = row.find_all('td')
                        if len(cells) >= 4:
                            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞—Ç—ã
                            date_cell = cells[0]
                            match_date = date_cell.get_text(strip=True)
                            
                            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏
                            time_cell = cells[1]
                            match_time = time_cell.get_text(strip=True)
                            
                            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–º–∞–Ω–¥
                            team1_cell = cells[2]
                            team2_cell = cells[3]
                            
                            team1 = team1_cell.get_text(strip=True)
                            team2 = team2_cell.get_text(strip=True)
                            
                            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ö–•–õ –∫–æ–º–∞–Ω–¥
                            if not (self.is_khl_team(team1) or self.is_khl_team(team2)):
                                continue
                            
                            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—á–µ—Ç–∞
                            score = ''
                            if len(cells) >= 5:
                                score_cell = cells[4]
                                score = score_cell.get_text(strip=True)
                            
                            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞
                            status = 'upcoming'
                            if score and ':' in score:
                                status = 'finished'
                            elif 'live' in match_time.lower():
                                status = 'live'
                            
                            match_data = {
                                'match_id': f"khl_{hash(team1 + team2 + match_date + match_time)}",
                                'team1': team1,
                                'team2': team2,
                                'tournament': 'KHL',
                                'sport': 'khl',
                                'date': f"{match_date} {match_time}".strip(),
                                'status': status,
                                'score': score if score else None,
                                'link': 'https://khl.ru/calendar/',
                                'source': 'khl_official'
                            }
                            matches.append(match_data)
                            
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è Error parsing KHL official row: {e}")
                        continue
                        
        except Exception as e:
            logger.error(f"‚ùå Error parsing KHL official matches: {e}")
        
        logger.info(f"üìä KHL Official: Found {len(matches)} matches")
        return matches
    
    async def parse_matches(self) -> List[Dict]:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        logger.info("üöÄ Starting KHL Parser (Fixed)")
        
        # –°–æ–∑–¥–∞–µ–º —Å–µ—Å—Å–∏—é
        self.session = aiohttp.ClientSession(timeout=self.session_timeout)
        
        try:
            all_matches = []
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
            sorted_sources = sorted(
                [(name, config) for name, config in self.sources.items() if config['enabled']],
                key=lambda x: x[1]['priority']
            )
            
            for source_name, source_config in sorted_sources:
                try:
                    logger.info(f"üìä Parsing {source_name}...")
                    
                    html = await self.fetch_page(source_config['url'], source_name)
                    if html:
                        if source_name == 'livesport':
                            matches = self.parse_livesport_matches(html)
                        elif source_name == 'khl_official':
                            matches = self.parse_khl_official_matches(html)
                        else:
                            continue
                        
                        all_matches.extend(matches)
                        
                        # –ï—Å–ª–∏ –ø–æ–ª—É—á–∏–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –º–∞—Ç—á–µ–π, –Ω–µ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º
                        if len(all_matches) >= 15:
                            logger.info(f"‚úÖ Got enough matches ({len(all_matches)}), stopping")
                            break
                    else:
                        logger.warning(f"‚ö†Ô∏è No data from {source_name}")
                        continue
                        
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Error parsing {source_name}: {e}")
                    continue
            
            # –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
            unique_matches = []
            seen_matches = set()
            
            for match in all_matches:
                match_key = f"{match['team1']}_{match['team2']}_{match['date']}"
                if match_key not in seen_matches:
                    unique_matches.append(match)
                    seen_matches.add(match_key)
            
            logger.info(f"‚úÖ KHL Parser completed: {len(unique_matches)} unique matches")
            return unique_matches
            
        except Exception as e:
            logger.error(f"‚ùå KHL Parser error: {e}")
            return []
        finally:
            if self.session:
                await self.session.close()
