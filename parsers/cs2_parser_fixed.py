#!/usr/bin/env python3
"""
AIBET CS2 Parser - Fixed Version
–¢–æ–ª—å–∫–æ HTML –ø–∞—Ä—Å–∏–Ω–≥ —Å Liquipedia –∏ HLTV.org
–ë–µ–∑ API, —Å –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º –≤ SQLite
"""

import asyncio
import aiohttp
import sqlite3
import json
import logging
import random
import time
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Any
from bs4 import BeautifulSoup
import re

logger = logging.getLogger(__name__)

class CS2ParserFixed:
    def __init__(self):
        self.db_path = "data/cs2_cache.db"
        self.session_timeout = aiohttp.ClientTimeout(total=15)
        self.max_retries = 3
        self.cache_hours = 6  # –ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ 6 —á–∞—Å–æ–≤
        
        # User-Agent –¥–ª—è –æ–±—Ö–æ–¥–∞ –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        ]
        
        # –ò—Å—Ç–æ—á–Ω–∏–∫–∏ (–¢–û–õ–¨–ö–û HTML)
        self.sources = {
            'liquipedia': {
                'url': 'https://liquipedia.net/counterstrike/Portal:Matches',
                'priority': 1,
                'enabled': True
            },
            'hltv': {
                'url': 'https://www.hltv.org/matches',
                'priority': 2,
                'enabled': True  # Fallback —Ç–æ–ª—å–∫–æ
            }
        }
        
        # –¢–æ–ø-30 CS2 –∫–æ–º–∞–Ω–¥ (–¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏)
        self.top_teams = [
            'NaVi', 'FaZe', 'G2', 'Vitality', 'Astralis', 'Heroic', 'Cloud9', 'Fnatic',
            'Team Liquid', 'Complexity', 'Evil Geniuses', 'FURIA', 'MOUZ', 'BIG', 'NIP',
            'ENCE', 'OG', 'Virtus.pro', 'forZe', '9INE', 'Imperial', '00 Nation', 'MIBR',
            'paiN', '9z', 'TYLOO', 'Lynn Vision', 'Rare Atom', 'Monte', 'B8', 'Sangal'
        ]
        
        self.session = None
        self._init_cache_db()
    
    def _init_cache_db(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è"""
        os.makedirs("data", exist_ok=True)
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS cache (
                source TEXT PRIMARY KEY,
                data TEXT,
                timestamp DATETIME,
                expires_at DATETIME
            )
        ''')
        
        conn.commit()
        conn.close()
        logger.info("‚úÖ CS2 cache database initialized")
    
    def _get_cached_data(self, source: str) -> Optional[Dict]:
        """–ü–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ –∫–µ—à–∞"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT data, expires_at FROM cache 
            WHERE source = ? AND expires_at > datetime('now')
        ''', (source,))
        
        result = cursor.fetchone()
        conn.close()
        
        if result:
            logger.info(f"üì¶ Using cached data for {source}")
            return json.loads(result[0])
        
        return None
    
    def _save_cached_data(self, source: str, data: Dict):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –≤ –∫–µ—à"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        expires_at = datetime.now() + timedelta(hours=self.cache_hours)
        
        cursor.execute('''
            INSERT OR REPLACE INTO cache (source, data, timestamp, expires_at)
            VALUES (?, ?, ?, ?)
        ''', (source, json.dumps(data), datetime.now(), expires_at))
        
        conn.commit()
        conn.close()
        logger.info(f"üíæ Cached data for {source} (expires: {expires_at})")
    
    def get_headers(self) -> Dict[str, str]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ª—É—á–∞–π–Ω—ã–µ headers"""
        return {
            'User-Agent': random.choice(self.user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9,ru;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Cache-Control': 'max-age=0'
        }
    
    async def fetch_page(self, url: str, source_name: str) -> Optional[str]:
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å retry –∏ –∑–∞–¥–µ—Ä–∂–∫–æ–π"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–µ—à
        cached = self._get_cached_data(source_name)
        if cached:
            return cached.get('html')
        
        for attempt in range(self.max_retries):
            try:
                # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                await asyncio.sleep(random.uniform(2, 4))
                
                headers = self.get_headers()
                
                async with self.session.get(url, headers=headers, timeout=self.session_timeout) as response:
                    if response.status == 200:
                        html = await response.text()
                        
                        # –ö–µ—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                        self._save_cached_data(source_name, {'html': html})
                        
                        logger.info(f"‚úÖ Successfully fetched {source_name}")
                        return html
                    elif response.status == 403:
                        logger.warning(f"‚ö†Ô∏è 403 Forbidden for {source_name} (attempt {attempt + 1})")
                        if attempt < self.max_retries - 1:
                            await asyncio.sleep(5)
                            continue
                        else:
                            # –ü—Ä–∏ 403 –æ—Ç HLTV - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫
                            if source_name == 'hltv':
                                logger.warning(f"‚ö†Ô∏è HLTV blocked, skipping...")
                                return None
                    elif response.status == 429:
                        logger.warning(f"‚ö†Ô∏è 429 Rate Limited for {source_name} (attempt {attempt + 1})")
                        if attempt < self.max_retries - 1:
                            await asyncio.sleep(10)
                            continue
                    else:
                        logger.warning(f"‚ö†Ô∏è HTTP {response.status} for {source_name}")
                        
            except asyncio.TimeoutError:
                logger.warning(f"‚ö†Ô∏è Timeout for {source_name} (attempt {attempt + 1})")
                if attempt < self.max_retries - 1:
                    await asyncio.sleep(3)
                    continue
            except Exception as e:
                logger.error(f"‚ùå Error fetching {source_name}: {e}")
                if attempt < self.max_retries - 1:
                    await asyncio.sleep(2)
                    continue
        
        logger.error(f"‚ùå Failed to fetch {source_name} after {self.max_retries} attempts")
        return None
    
    def is_top_team(self, team_name: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å, —á—Ç–æ –∫–æ–º–∞–Ω–¥–∞ –≤ —Ç–æ–ø–µ"""
        team_lower = team_name.lower()
        for top_team in self.top_teams:
            if top_team.lower() in team_lower:
                return True
        return False
    
    def parse_liquipedia_matches(self, html: str) -> List[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –º–∞—Ç—á–µ–π —Å Liquipedia"""
        matches = []
        
        try:
            soup = BeautifulSoup(html, 'html.parser')
            
            # Liquipedia –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–∞–±–ª–∏—Ü—ã –¥–ª—è –º–∞—Ç—á–µ–π
            match_table = soup.find('table', class_='infobox_matches_content')
            if not match_table:
                # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫
                match_table = soup.find('div', class_='matches-list')
            
            if match_table:
                rows = match_table.find_all('tr')
                
                for row in rows:
                    try:
                        cells = row.find_all('td')
                        if len(cells) >= 3:
                            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–º–∞–Ω–¥
                            team1_elem = cells[0].find('span', class_='team-template-text')
                            team2_elem = cells[2].find('span', class_='team-template-text')
                            
                            if not team1_elem or not team2_elem:
                                continue
                            
                            team1 = team1_elem.get_text(strip=True)
                            team2 = team2_elem.get_text(strip=True)
                            
                            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ç–æ–ø –∫–æ–º–∞–Ω–¥
                            if not (self.is_top_team(team1) or self.is_top_team(team2)):
                                continue
                            
                            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏
                            time_cell = cells[1]
                            time_elem = time_cell.find('span', class_='timer-object')
                            match_time = time_elem.get_text(strip=True) if time_elem else time_cell.get_text(strip=True)
                            
                            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç—É—Ä–Ω–∏—Ä–∞
                            tournament_elem = row.find('div', class_='tournament-text')
                            tournament = tournament_elem.get_text(strip=True) if tournament_elem else 'Unknown Tournament'
                            
                            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ñ–æ—Ä–º–∞—Ç–∞
                            format_elem = row.find('div', class_='match-format')
                            match_format = format_elem.get_text(strip=True) if format_elem else 'BO1'
                            
                            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Å—ã–ª–∫–∏
                            link_elem = row.find('a', href=True)
                            match_link = link_elem['href'] if link_elem else ''
                            
                            match_data = {
                                'match_id': f"liquipedia_{hash(team1 + team2 + match_time)}",
                                'team1': team1,
                                'team2': team2,
                                'tournament': tournament,
                                'sport': 'cs2',
                                'date': match_time,
                                'status': 'upcoming',
                                'format': match_format,
                                'link': match_link,
                                'source': 'liquipedia'
                            }
                            matches.append(match_data)
                            
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è Error parsing Liquipedia row: {e}")
                        continue
                        
        except Exception as e:
            logger.error(f"‚ùå Error parsing Liquipedia matches: {e}")
        
        logger.info(f"üìä Liquipedia: Found {len(matches)} top CS2 matches")
        return matches
    
    def parse_hltv_matches(self, html: str) -> List[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –º–∞—Ç—á–µ–π —Å HLTV.org (—Ç–æ–ª—å–∫–æ HTML)"""
        matches = []
        
        try:
            soup = BeautifulSoup(html, 'html.parser')
            
            # HLTV –∏—Å–ø–æ–ª—å–∑—É–µ—Ç div —Å –∫–ª–∞—Å—Å–æ–º match-day
            match_days = soup.find_all('div', class_='match-day')
            
            for match_day in match_days:
                try:
                    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞—Ç—ã
                    date_elem = match_day.find('div', class_='standard-headline')
                    match_date = date_elem.get_text(strip=True) if date_elem else 'Unknown'
                    
                    # –ü–æ–∏—Å–∫ –º–∞—Ç—á–µ–π –∑–∞ –¥–µ–Ω—å
                    match_elements = match_day.find_all('div', class_='match')
                    
                    for element in match_elements:
                        try:
                            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–º–∞–Ω–¥
                            team_elements = element.find_all('div', class_='team')
                            if len(team_elements) < 2:
                                continue
                            
                            team1 = team_elements[0].get_text(strip=True)
                            team2 = team_elements[1].get_text(strip=True)
                            
                            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ç–æ–ø –∫–æ–º–∞–Ω–¥
                            if not (self.is_top_team(team1) or self.is_top_team(team2)):
                                continue
                            
                            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏
                            time_elem = element.find('div', class_='time')
                            match_time = time_elem.get_text(strip=True) if time_elem else ''
                            
                            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç—É—Ä–Ω–∏—Ä–∞
                            event_elem = element.find('div', class_='event')
                            tournament = event_elem.get_text(strip=True) if event_elem else 'Unknown Tournament'
                            
                            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ñ–æ—Ä–º–∞—Ç–∞
                            format_elem = element.find('div', class_='best-of')
                            match_format = format_elem.get_text(strip=True) if format_elem else 'BO1'
                            
                            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Å—ã–ª–∫–∏
                            link_elem = element.find('a', href=True)
                            match_link = link_elem['href'] if link_elem else ''
                            
                            match_data = {
                                'match_id': f"hltv_{hash(team1 + team2 + match_time)}",
                                'team1': team1,
                                'team2': team2,
                                'tournament': tournament,
                                'sport': 'cs2',
                                'date': f"{match_date} {match_time}",
                                'status': 'upcoming',
                                'format': match_format,
                                'link': f"https://www.hltv.org{match_link}" if match_link.startswith('/') else match_link,
                                'source': 'hltv'
                            }
                            matches.append(match_data)
                            
                        except Exception as e:
                            logger.warning(f"‚ö†Ô∏è Error parsing HLTV match element: {e}")
                            continue
                            
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Error parsing HLTV match day: {e}")
                    continue
                            
        except Exception as e:
            logger.error(f"‚ùå Error parsing HLTV matches: {e}")
        
        logger.info(f"üìä HLTV: Found {len(matches)} top CS2 matches")
        return matches
    
    async def parse_matches(self) -> List[Dict]:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        logger.info("üöÄ Starting CS2 Parser (Fixed)")
        
        # –°–æ–∑–¥–∞–µ–º —Å–µ—Å—Å–∏—é
        self.session = aiohttp.ClientSession(timeout=self.session_timeout)
        
        try:
            all_matches = []
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
            sorted_sources = sorted(
                [(name, config) for name, config in self.sources.items() if config['enabled']],
                key=lambda x: x[1]['priority']
            )
            
            for source_name, source_config in sorted_sources:
                try:
                    logger.info(f"üìä Parsing {source_name}...")
                    
                    html = await self.fetch_page(source_config['url'], source_name)
                    if html:
                        if source_name == 'liquipedia':
                            matches = self.parse_liquipedia_matches(html)
                        elif source_name == 'hltv':
                            matches = self.parse_hltv_matches(html)
                        else:
                            continue
                        
                        all_matches.extend(matches)
                        
                        # –ï—Å–ª–∏ –ø–æ–ª—É—á–∏–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –º–∞—Ç—á–µ–π, –Ω–µ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º
                        if len(all_matches) >= 10:
                            logger.info(f"‚úÖ Got enough matches ({len(all_matches)}), stopping")
                            break
                    else:
                        logger.warning(f"‚ö†Ô∏è No data from {source_name}")
                        continue
                        
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Error parsing {source_name}: {e}")
                    continue
            
            # –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
            unique_matches = []
            seen_matches = set()
            
            for match in all_matches:
                match_key = f"{match['team1']}_{match['team2']}_{match['date']}"
                if match_key not in seen_matches:
                    unique_matches.append(match)
                    seen_matches.add(match_key)
            
            logger.info(f"‚úÖ CS2 Parser completed: {len(unique_matches)} unique matches")
            return unique_matches
            
        except Exception as e:
            logger.error(f"‚ùå CS2 Parser error: {e}")
            return []
        finally:
            if self.session:
                await self.session.close()

# –ò–º–ø–æ—Ä—Ç os –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
import os
