#!/usr/bin/env python3
"""
AIBET Analytics Platform - Fixed ML Models
–û–±—É—á–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ SQLite
"""

import asyncio
import logging
import pickle
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Tuple, Any
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import accuracy_score, classification_report
from sklearn.preprocessing import StandardScaler
import os

logger = logging.getLogger(__name__)

class MLModelsFixed:
    def __init__(self, db_manager):
        self.db_manager = db_manager
        self.models_path = "models"
        self.models = {}
        self.scalers = {}
        self.feature_columns = []
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –º–æ–¥–µ–ª–µ–π
        os.makedirs(self.models_path, exist_ok=True)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π
        self._init_models()
    
    def _init_models(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π"""
        self.models = {
            'logistic_regression': LogisticRegression(random_state=42, max_iter=1000),
            'random_forest': RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10)
        }
        
        self.scalers = {
            'logistic_regression': StandardScaler(),
            'random_forest': StandardScaler()
        }
        
        logger.info("‚úÖ ML models initialized")
    
    async def collect_training_data(self, sport: str, min_matches: int = 100) -> Optional[pd.DataFrame]:
        """–°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏–∑ –±–∞–∑—ã"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–µ –º–∞—Ç—á–∏
            matches = await self.db_manager.get_matches(sport=sport, status='finished')
            
            if len(matches) < min_matches:
                logger.warning(f"‚ö†Ô∏è Not enough matches for training: {len(matches)}/{min_matches}")
                return None
            
            logger.info(f"üìä Collecting training data: {len(matches)} matches")
            
            training_data = []
            
            for match in matches:
                try:
                    # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–æ–º–∞–Ω–¥
                    stats1 = await self.db_manager.get_team_stats(match['team1'], sport)
                    stats2 = await self.db_manager.get_team_stats(match['team2'], sport)
                    
                    if not stats1 or not stats2:
                        continue
                    
                    # –°–æ–∑–¥–∞–µ–º —Ñ–∏—á–∏
                    features = self._extract_features(stats1, stats2, match)
                    
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç (–¥–ª—è –æ–±—É—á–µ–Ω–∏—è)
                    winner = self._determine_winner(match)
                    if winner:
                        features['result'] = winner
                        training_data.append(features)
                        
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Error processing match {match['id']}: {e}")
                    continue
            
            if not training_data:
                logger.error("‚ùå No training data collected")
                return None
            
            df = pd.DataFrame(training_data)
            logger.info(f"‚úÖ Training data collected: {len(df)} samples")
            
            return df
            
        except Exception as e:
            logger.error(f"‚ùå Error collecting training data: {e}")
            return None
    
    def _extract_features(self, stats1: Dict, stats2: Dict, match: Dict) -> Dict:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ñ–∏—á–µ–π –∏–∑ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        features = {}
        
        # –ë–∞–∑–æ–≤—ã–µ —Ñ–∏—á–∏ –∫–æ–º–∞–Ω–¥—ã 1
        features['team1_win_rate'] = stats1.get('win_rate', 0.0)
        features['team1_matches_played'] = stats1.get('matches_played', 0)
        features['team1_wins'] = stats1.get('wins', 0)
        features['team1_losses'] = stats1.get('losses', 0)
        features['team1_draws'] = stats1.get('draws', 0)
        features['team1_score_for_avg'] = stats1.get('score_for', 0) / max(stats1.get('matches_played', 1), 1)
        features['team1_score_against_avg'] = stats1.get('score_against', 0) / max(stats1.get('matches_played', 1), 1)
        
        # –ë–∞–∑–æ–≤—ã–µ —Ñ–∏—á–∏ –∫–æ–º–∞–Ω–¥—ã 2
        features['team2_win_rate'] = stats2.get('win_rate', 0.0)
        features['team2_matches_played'] = stats2.get('matches_played', 0)
        features['team2_wins'] = stats2.get('wins', 0)
        features['team2_losses'] = stats2.get('losses', 0)
        features['team2_draws'] = stats2.get('draws', 0)
        features['team2_score_for_avg'] = stats2.get('score_for', 0) / max(stats2.get('matches_played', 1), 1)
        features['team2_score_against_avg'] = stats2.get('score_against', 0) / max(stats2.get('matches_played', 1), 1)
        
        # –†–∞–∑–Ω–æ—Å—Ç–Ω—ã–µ —Ñ–∏—á–∏
        features['win_rate_diff'] = features['team1_win_rate'] - features['team2_win_rate']
        features['matches_played_diff'] = features['team1_matches_played'] - features['team2_matches_played']
        features['score_for_diff'] = features['team1_score_for_avg'] - features['team2_score_for_avg']
        features['score_against_diff'] = features['team1_score_against_avg'] - features['team2_score_against_avg']
        
        # –§–∏—á–∏ —Ñ–æ—Ä–º—ã
        recent_form1 = stats1.get('recent_form', [])
        recent_form2 = stats2.get('recent_form', [])
        
        features['team1_recent_wins'] = recent_form1.count('W') if recent_form1 else 0
        features['team2_recent_wins'] = recent_form2.count('W') if recent_form2 else 0
        features['team1_recent_losses'] = recent_form1.count('L') if recent_form1 else 0
        features['team2_recent_losses'] = recent_form2.count('L') if recent_form2 else 0
        
        # –§–∏—á–∏ –º–∞—Ç—á–∞
        features['home_advantage'] = 1.0 if 'home' in match.get('team1', '').lower() else 0.0
        
        return features
    
    def _determine_winner(self, match: Dict) -> Optional[str]:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ–±–µ–¥–∏—Ç–µ–ª—è –º–∞—Ç—á–∞"""
        if not match.get('score'):
            return None
        
        try:
            score = match['score']
            if ':' in score:
                scores = score.split(':')
                if len(scores) == 2:
                    team1_score = int(scores[0].strip())
                    team2_score = int(scores[1].strip())
                    
                    if team1_score > team2_score:
                        return 'team1'
                    elif team2_score > team1_score:
                        return 'team2'
                    else:
                        return 'draw'
        except:
            pass
        
        return None
    
    async def train_models(self, sport: str = 'cs2') -> bool:
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π"""
        try:
            logger.info(f"ü§ñ Training ML models for {sport}")
            
            # –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö
            df = await self.collect_training_data(sport, min_matches=100)
            if df is None:
                logger.warning(f"‚ö†Ô∏è Not enough data for training {sport}")
                return False
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            X = df.drop(['result'], axis=1)
            y = df['result']
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è —Ñ–∏—á–µ–π
            self.feature_columns = X.columns.tolist()
            
            # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42, stratify=y
            )
            
            # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
            results = {}
            
            for model_name, model in self.models.items():
                try:
                    logger.info(f"üîß Training {model_name}")
                    
                    # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
                    X_train_scaled = self.scalers[model_name].fit_transform(X_train)
                    X_test_scaled = self.scalers[model_name].transform(X_test)
                    
                    # –û–±—É—á–µ–Ω–∏–µ
                    model.fit(X_train_scaled, y_train)
                    
                    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
                    y_pred = model.predict(X_test_scaled)
                    
                    # –û—Ü–µ–Ω–∫–∞
                    accuracy = accuracy_score(y_test, y_pred)
                    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5)
                    
                    results[model_name] = {
                        'accuracy': accuracy,
                        'cv_mean': cv_scores.mean(),
                        'cv_std': cv_scores.std(),
                        'report': classification_report(y_test, y_pred, output_dict=True)
                    }
                    
                    logger.info(f"‚úÖ {model_name}: Accuracy={accuracy:.3f}, CV={cv_scores.mean():.3f}¬±{cv_scores.std():.3f}")
                    
                    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
                    model_path = os.path.join(self.models_path, f"{sport}_{model_name}.pkl")
                    scaler_path = os.path.join(self.models_path, f"{sport}_{model_name}_scaler.pkl")
                    
                    with open(model_path, 'wb') as f:
                        pickle.dump(model, f)
                    
                    with open(scaler_path, 'wb') as f:
                        pickle.dump(self.scalers[model_name], f)
                    
                    logger.info(f"üíæ Model saved: {model_path}")
                    
                except Exception as e:
                    logger.error(f"‚ùå Error training {model_name}: {e}")
                    continue
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            metadata = {
                'sport': sport,
                'feature_columns': self.feature_columns,
                'training_date': datetime.now().isoformat(),
                'results': results,
                'samples_count': len(df)
            }
            
            metadata_path = os.path.join(self.models_path, f"{sport}_metadata.pkl")
            with open(metadata_path, 'wb') as f:
                pickle.dump(metadata, f)
            
            logger.info(f"‚úÖ ML models training completed for {sport}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Error training models: {e}")
            return False
    
    async def load_models(self, sport: str) -> bool:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        try:
            metadata_path = os.path.join(self.models_path, f"{sport}_metadata.pkl")
            
            if not os.path.exists(metadata_path):
                logger.warning(f"‚ö†Ô∏è No trained models found for {sport}")
                return False
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            with open(metadata_path, 'rb') as f:
                metadata = pickle.load(f)
            
            self.feature_columns = metadata['feature_columns']
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π
            for model_name in self.models.keys():
                model_path = os.path.join(self.models_path, f"{sport}_{model_name}.pkl")
                scaler_path = os.path.join(self.models_path, f"{sport}_{model_name}_scaler.pkl")
                
                if os.path.exists(model_path) and os.path.exists(scaler_path):
                    with open(model_path, 'rb') as f:
                        self.models[model_name] = pickle.load(f)
                    
                    with open(scaler_path, 'rb') as f:
                        self.scalers[model_name] = pickle.load(f)
                    
                    logger.info(f"‚úÖ Loaded {model_name} for {sport}")
                else:
                    logger.warning(f"‚ö†Ô∏è Model files not found for {model_name}")
            
            logger.info(f"‚úÖ Models loaded for {sport}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Error loading models: {e}")
            return False
    
    async def predict_match(self, match: Dict, sport: str) -> Dict[str, Any]:
        """–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –º–∞—Ç—á–∞"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
            if not self.feature_columns:
                # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª–∏
                if not await self.load_models(sport):
                    # –ï—Å–ª–∏ –Ω–µ—Ç –º–æ–¥–µ–ª–µ–π, –∏—Å–ø–æ–ª—å–∑—É–µ–º rule-based
                    return self._rule_based_prediction(match, sport)
            
            # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–æ–º–∞–Ω–¥
            stats1 = await self.db_manager.get_team_stats(match['team1'], sport)
            stats2 = await self.db_manager.get_team_stats(match['team2'], sport)
            
            if not stats1 or not stats2:
                return self._rule_based_prediction(match, sport)
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ñ–∏—á–µ–π
            features = self._extract_features(stats1, stats2, match)
            
            # –°–æ–∑–¥–∞–Ω–∏–µ DataFrame
            feature_df = pd.DataFrame([features])
            
            # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –≤—Å–µ —Ñ–∏—á–∏ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç
            for col in self.feature_columns:
                if col not in feature_df.columns:
                    feature_df[col] = 0
            
            # –£–ø–æ—Ä—è–¥–æ—á–∏–≤–∞–µ–º –∫–æ–ª–æ–Ω–∫–∏
            feature_df = feature_df[self.feature_columns]
            
            # –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ
            predictions = {}
            
            for model_name, model in self.models.items():
                try:
                    # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
                    X_scaled = self.scalers[model_name].transform(feature_df)
                    
                    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
                    pred = model.predict(X_scaled)[0]
                    proba = model.predict_proba(X_scaled)[0]
                    
                    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
                    classes = model.classes_
                    prob_dict = dict(zip(classes, proba))
                    
                    predictions[model_name] = {
                        'prediction': pred,
                        'probabilities': prob_dict
                    }
                    
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Error in {model_name} prediction: {e}")
                    continue
            
            if not predictions:
                return self._rule_based_prediction(match, sport)
            
            # –ê–Ω—Å–∞–º–±–ª—å –ø—Ä–æ–≥–Ω–æ–∑–æ–≤
            ensemble_pred = self._ensemble_predictions(predictions)
            
            return ensemble_pred
            
        except Exception as e:
            logger.error(f"‚ùå Error in prediction: {e}")
            return self._rule_based_prediction(match, sport)
    
    def _rule_based_prediction(self, match: Dict, sport: str) -> Dict[str, Any]:
        """Rule-based –ø—Ä–æ–≥–Ω–æ–∑ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–∞–∑–æ–≤–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        try:
            # –ü—Ä–æ—Å—Ç–∞—è –ª–æ–≥–∏–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ win rate
            team1_wr = 0.5  # Default
            team2_wr = 0.5
            
            # –ï—Å–ª–∏ –µ—Å—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ—ë
            if hasattr(self, 'db_manager'):
                # –≠—Ç–æ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –º–µ—Ç–æ–¥, –Ω–æ –≤ rule-based –º—ã –Ω–µ –º–æ–∂–µ–º await
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
                pass
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ–±–µ–¥–∏—Ç–µ–ª—è
            if abs(team1_wr - team2_wr) < 0.05:
                prediction = 'draw' if sport == 'khl' else 'team1'
                confidence = 0.5
            elif team1_wr > team2_wr:
                prediction = 'team1'
                confidence = 0.5 + (team1_wr - team2_wr)
            else:
                prediction = 'team2'
                confidence = 0.5 + (team2_wr - team1_wr)
            
            confidence = max(0.1, min(0.9, confidence))
            
            return {
                'prediction': prediction,
                'confidence': confidence,
                'team1_probability': confidence if prediction == 'team1' else (1 - confidence),
                'team2_probability': confidence if prediction == 'team2' else (1 - confidence),
                'draw_probability': 0.1 if sport == 'khl' else 0.0,
                'method': 'rule_based'
            }
            
        except Exception as e:
            logger.error(f"‚ùå Error in rule-based prediction: {e}")
            return {
                'prediction': 'team1',
                'confidence': 0.5,
                'team1_probability': 0.5,
                'team2_probability': 0.5,
                'draw_probability': 0.0,
                'method': 'fallback'
            }
    
    def _ensemble_predictions(self, predictions: Dict) -> Dict[str, Any]:
        """–ê–Ω—Å–∞–º–±–ª—å –ø—Ä–æ–≥–Ω–æ–∑–æ–≤"""
        try:
            # –ü—Ä–æ—Å—Ç–æ–µ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ
            votes = {}
            total_confidence = 0
            
            for model_name, pred in predictions.items():
                pred_class = pred['prediction']
                votes[pred_class] = votes.get(pred_class, 0) + 1
                
                # –£—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
                if 'probabilities' in pred:
                    for outcome, prob in pred['probabilities'].items():
                        total_confidence += prob
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ–±–µ–¥–∏—Ç–µ–ª—è –ø–æ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏—é
            if not votes:
                return self._rule_based_prediction({}, 'cs2')
            
            winner = max(votes.keys(), key=lambda x: votes[x])
            
            # –£—Å—Ä–µ–¥–Ω–µ–Ω–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å
            avg_confidence = total_confidence / len(predictions) / len(predictions)
            
            return {
                'prediction': winner,
                'confidence': avg_confidence,
                'method': 'ensemble',
                'models_used': list(predictions.keys())
            }
            
        except Exception as e:
            logger.error(f"‚ùå Error in ensemble: {e}")
            return self._rule_based_prediction({}, 'cs2')
