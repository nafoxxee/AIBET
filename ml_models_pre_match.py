#!/usr/bin/env python3
"""
AIBET Analytics Platform - Pre-Match ML Models
ML –º–æ–¥–µ–ª–∏ –¥–ª—è pre-match –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
"""

import asyncio
import logging
import pickle
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Tuple, Any
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import accuracy_score, classification_report
from sklearn.preprocessing import StandardScaler
import os
import random

logger = logging.getLogger(__name__)

class PreMatchMLModels:
    def __init__(self, db_manager):
        self.db_manager = db_manager
        self.models_path = "models_pre_match"
        self.models = {}
        self.scalers = {}
        self.feature_columns = []
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –º–æ–¥–µ–ª–µ–π
        os.makedirs(self.models_path, exist_ok=True)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π
        self._init_models()
    
    def _init_models(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π"""
        self.models = {
            'logistic_regression': LogisticRegression(random_state=42, max_iter=1000),
            'random_forest': RandomForestClassifier(n_estimators=100, random_state=42, max_depth=8)
        }
        
        self.scalers = {
            'logistic_regression': StandardScaler(),
            'random_forest': StandardScaler()
        }
        
        logger.info("‚úÖ Pre-Match ML models initialized")
    
    async def collect_training_data(self, sport: str, min_matches: int = 50) -> Optional[pd.DataFrame]:
        """–°–±–æ—Ä –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –º–∞—Ç—á–∏
            matches = await self.db_manager.get_historical_matches(sport=sport, limit=500)
            
            if len(matches) < min_matches:
                logger.warning(f"‚ö†Ô∏è Not enough historical matches for training: {len(matches)}/{min_matches}")
                return None
            
            logger.info(f"üìä Collecting historical training data: {len(matches)} matches")
            
            training_data = []
            
            for match in matches:
                try:
                    # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–æ–º–∞–Ω–¥
                    stats1 = await self.db_manager.get_team_stats(match['team1'], sport)
                    stats2 = await self.db_manager.get_team_stats(match['team2'], sport)
                    
                    if not stats1 or not stats2:
                        # –°–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –µ—Å–ª–∏ –Ω–µ—Ç
                        stats1 = self._create_basic_stats(match['team1'], sport)
                        stats2 = self._create_basic_stats(match['team2'], sport)
                    
                    # –°–æ–∑–¥–∞–µ–º —Ñ–∏—á–∏
                    features = self._extract_pre_match_features(stats1, stats2, match)
                    
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                    result = match.get('result', 'unknown')
                    if result in ['team1', 'team2', 'draw']:
                        features['result'] = result
                        training_data.append(features)
                        
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Error processing historical match {match['id']}: {e}")
                    continue
            
            if not training_data:
                logger.error("‚ùå No training data collected")
                return None
            
            df = pd.DataFrame(training_data)
            logger.info(f"‚úÖ Historical training data collected: {len(df)} samples")
            
            return df
            
        except Exception as e:
            logger.error(f"‚ùå Error collecting historical training data: {e}")
            return None
    
    def _create_basic_stats(self, team_name: str, sport: str) -> Dict:
        """–°–æ–∑–¥–∞—Ç—å –±–∞–∑–æ–≤—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–ª—è –∫–æ–º–∞–Ω–¥—ã"""
        return {
            'matches_played': 30,
            'wins': random.randint(12, 20),
            'losses': 0,
            'draws': 0,
            'goals_for': random.randint(50, 100) if sport == 'khl' else random.randint(500, 800),
            'goals_against': random.randint(40, 90) if sport == 'khl' else random.randint(400, 700),
            'recent_form': ['W', 'L', 'W', 'W', 'L'],
            'win_rate': 0.55
        }
    
    def _extract_pre_match_features(self, stats1: Dict, stats2: Dict, match: Dict) -> Dict:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ pre-match —Ñ–∏—á–µ–π"""
        features = {}
        
        # –ë–∞–∑–æ–≤—ã–µ —Ñ–∏—á–∏ –∫–æ–º–∞–Ω–¥—ã 1
        features['team1_win_rate'] = stats1.get('win_rate', 0.5)
        features['team1_matches_played'] = stats1.get('matches_played', 30)
        features['team1_wins'] = stats1.get('wins', 15)
        features['team1_losses'] = stats1.get('losses', 15)
        features['team1_draws'] = stats1.get('draws', 0)
        
        # –ë–∞–∑–æ–≤—ã–µ —Ñ–∏—á–∏ –∫–æ–º–∞–Ω–¥—ã 2
        features['team2_win_rate'] = stats2.get('win_rate', 0.5)
        features['team2_matches_played'] = stats2.get('matches_played', 30)
        features['team2_wins'] = stats2.get('wins', 15)
        features['team2_losses'] = stats2.get('losses', 15)
        features['team2_draws'] = stats2.get('draws', 0)
        
        # –†–∞–∑–Ω–æ—Å—Ç–Ω—ã–µ —Ñ–∏—á–∏
        features['win_rate_diff'] = features['team1_win_rate'] - features['team2_win_rate']
        features['matches_diff'] = features['team1_matches_played'] - features['team2_matches_played']
        features['wins_diff'] = features['team1_wins'] - features['team2_wins']
        
        # –§–∏—á–∏ —Ñ–æ—Ä–º—ã
        recent_form1 = stats1.get('recent_form', [])
        recent_form2 = stats2.get('recent_form', [])
        
        features['team1_recent_wins'] = recent_form1.count('W') if recent_form1 else 0
        features['team2_recent_wins'] = recent_form2.count('W') if recent_form2 else 0
        features['team1_recent_losses'] = recent_form1.count('L') if recent_form1 else 0
        features['team2_recent_losses'] = recent_form2.count('L') if recent_form2 else 0
        
        # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Ñ–∏—á–∏ –¥–ª—è —Å–ø–æ—Ä—Ç–∞
        if match['sport'] == 'khl':
            features['team1_goals_for_avg'] = stats1.get('goals_for', 100) / max(stats1.get('matches_played', 30), 1)
            features['team2_goals_for_avg'] = stats2.get('goals_for', 100) / max(stats2.get('matches_played', 30), 1)
            features['team1_goals_against_avg'] = stats1.get('goals_against', 90) / max(stats1.get('matches_played', 30), 1)
            features['team2_goals_against_avg'] = stats2.get('goals_against', 90) / max(stats2.get('matches_played', 30), 1)
            features['goals_diff'] = features['team1_goals_for_avg'] - features['team2_goals_for_avg']
        else:
            # CS2 —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Ñ–∏—á–∏
            features['team1_rank'] = match.get('features', {}).get('team1_rank', 10)
            features['team2_rank'] = match.get('features', {}).get('team2_rank', 10)
            features['rank_diff'] = features['team1_rank'] - features['team2_rank']
        
        return features
    
    async def train_models(self, sport: str = 'cs2') -> bool:
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –Ω–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö"""
        try:
            logger.info(f"ü§ñ Training Pre-Match ML models for {sport}")
            
            # –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö
            df = await self.collect_training_data(sport, min_matches=30)
            if df is None:
                logger.warning(f"‚ö†Ô∏è Not enough historical data for training {sport}")
                return False
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            X = df.drop(['result'], axis=1)
            y = df['result']
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è —Ñ–∏—á–µ–π
            self.feature_columns = X.columns.tolist()
            
            # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42, stratify=y
            )
            
            # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
            results = {}
            
            for model_name, model in self.models.items():
                try:
                    logger.info(f"üîß Training {model_name}")
                    
                    # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
                    X_train_scaled = self.scalers[model_name].fit_transform(X_train)
                    X_test_scaled = self.scalers[model_name].transform(X_test)
                    
                    # –û–±—É—á–µ–Ω–∏–µ
                    model.fit(X_train_scaled, y_train)
                    
                    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
                    y_pred = model.predict(X_test_scaled)
                    
                    # –û—Ü–µ–Ω–∫–∞
                    accuracy = accuracy_score(y_test, y_pred)
                    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=3)
                    
                    results[model_name] = {
                        'accuracy': accuracy,
                        'cv_mean': cv_scores.mean(),
                        'cv_std': cv_scores.std()
                    }
                    
                    logger.info(f"‚úÖ {model_name}: Accuracy={accuracy:.3f}, CV={cv_scores.mean():.3f}¬±{cv_scores.std():.3f}")
                    
                    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
                    model_path = os.path.join(self.models_path, f"{sport}_{model_name}.pkl")
                    scaler_path = os.path.join(self.models_path, f"{sport}_{model_name}_scaler.pkl")
                    
                    with open(model_path, 'wb') as f:
                        pickle.dump(model, f)
                    
                    with open(scaler_path, 'wb') as f:
                        pickle.dump(self.scalers[model_name], f)
                    
                    logger.info(f"üíæ Model saved: {model_path}")
                    
                except Exception as e:
                    logger.error(f"‚ùå Error training {model_name}: {e}")
                    continue
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            metadata = {
                'sport': sport,
                'feature_columns': self.feature_columns,
                'training_date': datetime.now().isoformat(),
                'results': results,
                'samples_count': len(df)
            }
            
            metadata_path = os.path.join(self.models_path, f"{sport}_metadata.pkl")
            with open(metadata_path, 'wb') as f:
                pickle.dump(metadata, f)
            
            logger.info(f"‚úÖ Pre-Match ML models training completed for {sport}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Error training pre-match models: {e}")
            return False
    
    async def load_models(self, sport: str) -> bool:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        try:
            metadata_path = os.path.join(self.models_path, f"{sport}_metadata.pkl")
            
            if not os.path.exists(metadata_path):
                logger.warning(f"‚ö†Ô∏è No trained pre-match models found for {sport}")
                return False
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            with open(metadata_path, 'rb') as f:
                metadata = pickle.load(f)
            
            self.feature_columns = metadata['feature_columns']
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π
            for model_name in self.models.keys():
                model_path = os.path.join(self.models_path, f"{sport}_{model_name}.pkl")
                scaler_path = os.path.join(self.models_path, f"{sport}_{model_name}_scaler.pkl")
                
                if os.path.exists(model_path) and os.path.exists(scaler_path):
                    with open(model_path, 'rb') as f:
                        self.models[model_name] = pickle.load(f)
                    
                    with open(scaler_path, 'rb') as f:
                        self.scalers[model_name] = pickle.load(f)
                    
                    logger.info(f"‚úÖ Loaded {model_name} for {sport}")
                else:
                    logger.warning(f"‚ö†Ô∏è Model files not found for {model_name}")
            
            logger.info(f"‚úÖ Pre-Match models loaded for {sport}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Error loading pre-match models: {e}")
            return False
    
    async def predict_match(self, match: Dict, sport: str) -> Dict[str, Any]:
        """–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ pre-match –º–∞—Ç—á–∞"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
            if not self.feature_columns:
                # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª–∏
                if not await self.load_models(sport):
                    # –ï—Å–ª–∏ –Ω–µ—Ç –º–æ–¥–µ–ª–µ–π, –∏—Å–ø–æ–ª—å–∑—É–µ–º rule-based
                    return self._rule_based_prediction(match, sport)
            
            # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–æ–º–∞–Ω–¥
            stats1 = await self.db_manager.get_team_stats(match['team1'], sport)
            stats2 = await self.db_manager.get_team_stats(match['team2'], sport)
            
            if not stats1:
                stats1 = self._create_basic_stats(match['team1'], sport)
            if not stats2:
                stats2 = self._create_basic_stats(match['team2'], sport)
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ñ–∏—á–µ–π
            features = self._extract_pre_match_features(stats1, stats2, match)
            
            # –°–æ–∑–¥–∞–Ω–∏–µ DataFrame
            feature_df = pd.DataFrame([features])
            
            # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –≤—Å–µ —Ñ–∏—á–∏ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç
            for col in self.feature_columns:
                if col not in feature_df.columns:
                    feature_df[col] = 0
            
            # –£–ø–æ—Ä—è–¥–æ—á–∏–≤–∞–µ–º –∫–æ–ª–æ–Ω–∫–∏
            feature_df = feature_df[self.feature_columns]
            
            # –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ
            predictions = {}
            
            for model_name, model in self.models.items():
                try:
                    # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
                    X_scaled = self.scalers[model_name].transform(feature_df)
                    
                    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
                    pred = model.predict(X_scaled)[0]
                    proba = model.predict_proba(X_scaled)[0]
                    
                    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
                    classes = model.classes_
                    prob_dict = dict(zip(classes, proba))
                    
                    predictions[model_name] = {
                        'prediction': pred,
                        'probabilities': prob_dict
                    }
                    
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Error in {model_name} prediction: {e}")
                    continue
            
            if not predictions:
                return self._rule_based_prediction(match, sport)
            
            # –ê–Ω—Å–∞–º–±–ª—å –ø—Ä–æ–≥–Ω–æ–∑–æ–≤
            ensemble_pred = self._ensemble_predictions(predictions)
            
            return {
                'prediction': ensemble_pred['prediction'],
                'confidence': ensemble_pred['confidence'],
                'team1_probability': ensemble_pred.get('team1_probability', 0.5),
                'team2_probability': ensemble_pred.get('team2_probability', 0.5),
                'draw_probability': ensemble_pred.get('draw_probability', 0.0),
                'method': 'pre_match_ensemble',
                'analysis_type': 'pre_match'
            }
            
        except Exception as e:
            logger.error(f"‚ùå Error in pre-match prediction: {e}")
            return self._rule_based_prediction(match, sport)
    
    def _rule_based_prediction(self, match: Dict, sport: str) -> Dict[str, Any]:
        """Rule-based –ø—Ä–æ–≥–Ω–æ–∑ –¥–ª—è pre-match"""
        try:
            # –ü—Ä–æ—Å—Ç–∞—è –ª–æ–≥–∏–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–∞–∑–æ–≤—ã—Ö —Ñ–∞–∫—Ç–æ—Ä–æ–≤
            team1_strength = 0.5
            team2_strength = 0.5
            
            # –£—á–∏—Ç—ã–≤–∞–µ–º —Ç—É—Ä–Ω–∏—Ä (–µ—Å–ª–∏ –∏–∑–≤–µ—Å—Ç–µ–Ω)
            tournament = match.get('tournament', '').lower()
            if 'major' in tournament or 'premier' in tournament:
                # –í –∫—Ä—É–ø–Ω—ã—Ö —Ç—É—Ä–Ω–∏—Ä–∞—Ö —Ç–æ–ø –∫–æ–º–∞–Ω–¥—ã —Å–∏–ª—å–Ω–µ–µ
                if any(top in match['team1'].lower() for top in ['navi', 'faze', 'g2']):
                    team1_strength += 0.1
                if any(top in match['team2'].lower() for top in ['navi', 'faze', 'g2']):
                    team2_strength += 0.1
            
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
            total_strength = team1_strength + team2_strength
            team1_prob = team1_strength / total_strength
            team2_prob = team2_strength / total_strength
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ–±–µ–¥–∏—Ç–µ–ª—è
            if abs(team1_prob - team2_prob) < 0.05:
                prediction = 'draw' if sport == 'khl' else 'team1'
                confidence = 0.5
            elif team1_prob > team2_prob:
                prediction = 'team1'
                confidence = team1_prob
            else:
                prediction = 'team2'
                confidence = team2_prob
            
            return {
                'prediction': prediction,
                'confidence': confidence,
                'team1_probability': team1_prob,
                'team2_probability': team2_prob,
                'draw_probability': 0.1 if sport == 'khl' else 0.0,
                'method': 'pre_match_rule_based',
                'analysis_type': 'pre_match'
            }
            
        except Exception as e:
            logger.error(f"‚ùå Error in pre-match rule-based prediction: {e}")
            return {
                'prediction': 'team1',
                'confidence': 0.5,
                'team1_probability': 0.5,
                'team2_probability': 0.5,
                'draw_probability': 0.0,
                'method': 'pre_match_fallback',
                'analysis_type': 'pre_match'
            }
    
    def _ensemble_predictions(self, predictions: Dict) -> Dict:
        """–ê–Ω—Å–∞–º–±–ª—å –ø—Ä–æ–≥–Ω–æ–∑–æ–≤"""
        try:
            # –ü—Ä–æ—Å—Ç–æ–µ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ
            votes = {}
            total_confidence = 0
            
            for model_name, pred in predictions.items():
                pred_class = pred['prediction']
                votes[pred_class] = votes.get(pred_class, 0) + 1
                
                # –£—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
                if 'probabilities' in pred:
                    for outcome, prob in pred['probabilities'].items():
                        total_confidence += prob
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ–±–µ–¥–∏—Ç–µ–ª—è –ø–æ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏—é
            if not votes:
                return {
                    'prediction': 'team1',
                    'confidence': 0.5,
                    'team1_probability': 0.5,
                    'team2_probability': 0.5,
                    'draw_probability': 0.0
                }
            
            winner = max(votes.keys(), key=lambda x: votes[x])
            
            # –£—Å—Ä–µ–¥–Ω–µ–Ω–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å
            avg_confidence = total_confidence / len(predictions) / len(predictions)
            
            result = {
                'prediction': winner,
                'confidence': avg_confidence,
                'team1_probability': avg_confidence if winner == 'team1' else (1 - avg_confidence),
                'team2_probability': avg_confidence if winner == 'team2' else (1 - avg_confidence),
                'draw_probability': 0.0
            }
            
            # –î–æ–±–∞–≤–ª—è–µ–º draw –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥–ª—è –ö–•–õ
            if 'draw' in predictions.get('logistic_regression', {}).get('probabilities', {}):
                result['draw_probability'] = predictions['logistic_regression']['probabilities']['draw']
            
            return result
            
        except Exception as e:
            logger.error(f"‚ùå Error in pre-match ensemble: {e}")
            return {
                'prediction': 'team1',
                'confidence': 0.5,
                'team1_probability': 0.5,
                'team2_probability': 0.5,
                'draw_probability': 0.0
            }
